{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNwL3hQCmzSe"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KONeeg6_3zai"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# ! pip install -q equinox optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxIgUReu4ICP"
   },
   "outputs": [],
   "source": [
    "# ! git remote remove origin\n",
    "# ! git init .\n",
    "# ! git remote add origin https://github.com/arudikov/PNO\n",
    "# ! git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCDCkU_64Krn"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSxXHakg4mp3"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "import itertools\n",
    "import sympy as sp\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets.Elliptic import solvers\n",
    "from architectures.DilResNet import DilatedResNet\n",
    "from architectures.FNO import FNO\n",
    "from architectures.DilResNet import make_step as make_step_Dil\n",
    "from architectures.FNO import make_step as make_step_FNO\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "from jax import config, random, grad, jit, vmap\n",
    "from jax.lax import scan\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIXBEDCil8ec"
   },
   "source": [
    "## Useful Code: `batch generator`, `compute_loss`, `train_on_epoch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMRziMI_aMKB"
   },
   "outputs": [],
   "source": [
    "def batch_generator(x, y, batch_size, key, shuffle=True):\n",
    "    N_samples = len(x)\n",
    "    list_of_indeces = jnp.linspace(0, N_samples-1, N_samples, dtype=jnp.int64)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(key, list_of_indeces)\n",
    "\n",
    "    list_x = x[list_of_indeces]\n",
    "    list_y = y[list_of_indeces]\n",
    "\n",
    "    n_batches = N_samples // batch_size\n",
    "    if N_samples % batch_size != 0:\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in range(n_batches):\n",
    "        this_batch_size = batch_size\n",
    "\n",
    "        if k == n_batches - 1:\n",
    "            if N_samples % batch_size > 0:\n",
    "                this_batch_size = N_samples % batch_size\n",
    "\n",
    "        x = jnp.array(list_x[k * batch_size : k * batch_size + this_batch_size])\n",
    "        y = jnp.array(list_y[k * batch_size : k * batch_size + this_batch_size])\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqz0apDxi2PG"
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, input, target):\n",
    "    output = model(input)\n",
    "    diff_norm = jnp.linalg.norm((output - target).reshape(input.shape[0], -1), axis=1)\n",
    "    y_norm = jnp.linalg.norm(target.reshape(input.shape[0], -1), axis=1)\n",
    "    return jnp.mean(diff_norm / y_norm)\n",
    "\n",
    "compute_loss_and_grads = eqx.filter_value_and_grad(compute_loss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXD1_UXbkS62"
   },
   "outputs": [],
   "source": [
    "def train_on_epoch(train_generator, model, make_step, optimizer, opt_state, n_iter):\n",
    "    epoch_loss = []\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss, model, opt_state = make_step(model, batch_of_x, batch_of_y, optimizer, opt_state)  \n",
    "        epoch_loss.append(batch_loss.item())\n",
    "        \n",
    "        n_iter += 1\n",
    "        \n",
    "    return epoch_loss, model, opt_state, n_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KghygmyYmLlj"
   },
   "source": [
    "## Generate Dataset for `train` and `validations` parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTOcnEj3_z4l"
   },
   "outputs": [],
   "source": [
    "def dataset(N_points, key):\n",
    "    coeffs = []\n",
    "    for key in iter(random.split(key, 3)):\n",
    "        coeffs.append(random.normal(key, (5,), dtype=jnp.complex128))\n",
    "\n",
    "    a = lambda x: -(jnp.real(0.5 * jnp.sum(jnp.stack([jnp.exp(1j * 2 * jnp.pi * x * k) * coeffs[0][k] for k in range(coeffs[0].size)], 0), 0))**2 + 0.5)\n",
    "    d = lambda x: jnp.zeros_like(x)\n",
    "    c = lambda x: jnp.real(0.2 * jnp.sum(jnp.stack([jnp.exp(1j * 2 * jnp.pi * x * k) * coeffs[1][k] for k in range(coeffs[1].size)], 0), 0))\n",
    "    f = lambda x: jnp.real(jnp.sum(jnp.stack([jnp.exp(1j * 2 * jnp.pi * x * k) * coeffs[2][k] for k in range(coeffs[2].size)], 0), 0))\n",
    "\n",
    "    F = [a, d, c, f]\n",
    "    BCs = [0, 0]\n",
    "    \n",
    "    x = jnp.linspace(0, 1, N_points)\n",
    "    features = jnp.vstack((-a(x), c(x), f(x)))\n",
    "    solution = solvers.solve_BVP(N_points, F, BCs)\n",
    "\n",
    "    return features, solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jDI_RZ7__qI"
   },
   "outputs": [],
   "source": [
    "def train_dataset(N_samples, N_points=100):\n",
    "    features, targets = [], []\n",
    "    \n",
    "    for key in tqdm(iter(random.split(random.PRNGKey(42), N_samples))):\n",
    "        feature, solution = dataset(N_points, key)\n",
    "        features.append(feature)\n",
    "        targets.append(solution)\n",
    "\n",
    "    return [jnp.array(features), jnp.array(targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CswlpIO1Tiai"
   },
   "outputs": [],
   "source": [
    "def validation_dataset(N_samples, N_points=100):\n",
    "    features, targets = [], []\n",
    "    for it in tqdm(random.randint(random.PRNGKey(10), (1,N_samples), 0,10000)[0]):\n",
    "        coeffs = []\n",
    "        for key in iter(random.split(random.PRNGKey(it), 3)):\n",
    "            coeffs.append(random.normal(key, (5,), dtype=jnp.complex128))\n",
    "\n",
    "        a = lambda x: -(jnp.real(0.5 * jnp.sum(jnp.stack([jnp.exp(1j * 2 * jnp.pi * x * k) * coeffs[0][k] for k in range(coeffs[0].size)], 0), 0))**2 + 0.5)\n",
    "        d = lambda x: jnp.zeros_like(x)\n",
    "        c = lambda x: jnp.real(0.2 * jnp.sum(jnp.stack([jnp.exp(1j * 2 * jnp.pi * x * k) * coeffs[1][k] for k in range(coeffs[1].size)], 0), 0))\n",
    "        f = lambda x: jnp.real(jnp.sum(jnp.stack([jnp.exp(1j * 2 * jnp.pi * x * k) * coeffs[2][k] for k in range(coeffs[2].size)], 0), 0))\n",
    "\n",
    "        F = [a, d, c, f]\n",
    "        BCs = [0, 0]\n",
    "        \n",
    "        x = jnp.linspace(0, 1, N_points)\n",
    "        feature = jnp.vstack((-a(x), c(x), f(x)))\n",
    "        solution = solvers.solve_BVP(N_points, F, BCs)\n",
    "\n",
    "        features.append(feature)\n",
    "        targets.append(solution)\n",
    "    \n",
    "    return [jnp.array(features), jnp.array(targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wb9oPoqFb2Tq"
   },
   "outputs": [],
   "source": [
    "def generator(key, N_points=100):\n",
    "    coeffs = []\n",
    "    for key in iter(random.split(key, 3)):\n",
    "        coeffs.append(random.normal(key, (5,), dtype=jnp.complex128))\n",
    "\n",
    "    a = lambda x: -(jnp.real(0.5 * jnp.sum(jnp.stack([jnp.exp(1j * 2 * jnp.pi * x * k) * coeffs[0][k] for k in range(coeffs[0].size)], 0), 0))**2 + 0.5)\n",
    "    d = lambda x: jnp.zeros_like(x)\n",
    "    c = lambda x: jnp.real(0.2 * jnp.sum(jnp.stack([jnp.exp(1j * 2 * jnp.pi * x * k) * coeffs[1][k] for k in range(coeffs[1].size)], 0), 0))\n",
    "    f = lambda x: jnp.real(jnp.sum(jnp.stack([jnp.exp(1j * 2 * jnp.pi * x * k) * coeffs[2][k] for k in range(coeffs[2].size)], 0), 0))\n",
    "\n",
    "    x = jnp.linspace(0, 1, N_points)\n",
    "    features = jnp.vstack((-a(x), c(x), f(x)))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CgCqZPtmcXl"
   },
   "source": [
    "## Define basic parameters of model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZirAKkwKzSG"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, model_name, params_of_model, params_of_learning):\n",
    "        if model_name == 'DilResNet':\n",
    "            self.model = DilatedResNet(key = random.PRNGKey(42), **params_of_model)\n",
    "            self.make_step = make_step_Dil\n",
    "\n",
    "        elif model_name == 'FNO':\n",
    "            self.model = FNO(**params_of_model, key=random.PRNGKey(42))\n",
    "            self.make_step = make_step_FNO\n",
    "\n",
    "        self.batch_size = params_of_learning['batch_size']\n",
    "        self.lr = params_of_learning['learning_rate']\n",
    "        self.count_of_epoch = params_of_learning['epochs']\n",
    "        self.fine_epoch = params_of_learning['finetuning_epochs']\n",
    "        self.fine_lr = params_of_learning['finetuning_lr']\n",
    "\n",
    "    def trainer(self, dataset, plot=True):\n",
    "        loss = compute_loss(self.model, dataset[0], dataset[1])\n",
    "        self.history = [loss.item(), ]\n",
    "\n",
    "        c = dataset[0].shape[0] // self.batch_size\n",
    "        dict_lr = {50 * c : 0.5, 100 * c : 0.5, 150 * c : 0.5, 200 * c : 0.5, \n",
    "                  250 * c : 0.5, 300 * c : 0.5, 350 * c : 0.5, 400 * c : 0.5, \n",
    "                  450 * c : 0.5, 500 * c : 0.5, 550 * c : 0.5, 600 * c : 0.5, \n",
    "                  650 * c : 0.5, 700 * c : 0.5, 750 * c : 0.5, 800 * c : 0.5, \n",
    "                  850 * c : 0.5, 900 * c : 0.5, 950 * c : 0.5, 1000 * c : 0.5}\n",
    "\n",
    "        sc = optax.piecewise_constant_schedule(self.lr, dict_lr)\n",
    "        optimizer = optax.experimental.split_real_and_imaginary(optax.adamw(sc, weight_decay=1e-2))\n",
    "        opt_state = optimizer.init(eqx.filter(self.model, eqx.is_array))\n",
    "\n",
    "        iterations = tqdm(range(self.count_of_epoch), desc='epoch')\n",
    "        iterations.set_postfix({'train epoch loss': jnp.nan})\n",
    "        n_iter = 0\n",
    "\n",
    "        for it in iterations:\n",
    "            key = random.PRNGKey(it)\n",
    "            generator = batch_generator(x=dataset[0], y=dataset[1], batch_size = self.batch_size, key = key, shuffle = True)\n",
    "            epoch_loss, self.model, opt_state, n_iter = train_on_epoch(train_generator = generator, \n",
    "                                                                  model = self.model, \n",
    "                                                                  make_step = self.make_step,                                          \n",
    "                                                                  optimizer = optimizer,\n",
    "                                                                  opt_state = opt_state,\n",
    "                                                                  n_iter = n_iter)\n",
    "            \n",
    "            iterations.set_postfix({'train epoch loss': epoch_loss})\n",
    "            self.history.append(jnp.array(epoch_loss).mean())\n",
    "            \n",
    "            if plot:\n",
    "                display.clear_output(wait=True)\n",
    "                fig = plt.figure(figsize=(10, 5))\n",
    "                plt.title(r'Loss')\n",
    "                plt.yscale(\"log\")\n",
    "                plt.plot(self.history, color='red', label='train')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def fine_trainer(self, dataset_fine, dataset, plot=True):\n",
    "        dataset = [jnp.concatenate([dataset_fine[0], dataset[0]],axis=0), jnp.concatenate([dataset_fine[1], dataset[1]],axis=0)]\n",
    "        loss = compute_loss(self.model, dataset[0], dataset[1])\n",
    "        self.history.append(loss.item())\n",
    "\n",
    "        optimizer = optax.experimental.split_real_and_imaginary(optax.adamw(self.fine_lr, weight_decay=1e-2))\n",
    "        opt_state = optimizer.init(eqx.filter(self.model, eqx.is_array))\n",
    "\n",
    "        iterations = tqdm(range(self.fine_epoch), desc='epoch')\n",
    "        iterations.set_postfix({'train epoch loss': jnp.nan})\n",
    "        n_iter = 0\n",
    "\n",
    "        for it in iterations:\n",
    "            key = random.PRNGKey(it)\n",
    "            generator = batch_generator(x=dataset_fine[0], y=dataset_fine[1], batch_size = self.batch_size, key = key, shuffle = True)\n",
    "            epoch_loss, self.model, opt_state, n_iter = train_on_epoch(train_generator = generator, \n",
    "                                                                  model = self.model, \n",
    "                                                                  make_step = self.make_step,                                          \n",
    "                                                                  optimizer = optimizer,\n",
    "                                                                  opt_state = opt_state,\n",
    "                                                                  n_iter = n_iter)\n",
    "            \n",
    "            iterations.set_postfix({'train epoch loss': epoch_loss})\n",
    "            loss = compute_loss(self.model, dataset[0], dataset[1])\n",
    "            self.history.append(loss.item())\n",
    "            \n",
    "            if plot:\n",
    "                display.clear_output(wait=True)\n",
    "                fig = plt.figure(figsize=(10, 5))\n",
    "                plt.title(r'Loss')\n",
    "                plt.yscale(\"log\")\n",
    "                plt.plot(self.history, color='red', label='train')\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TJRqkNBhdeT"
   },
   "source": [
    "## Upper bound, Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhLYsLTThj3e"
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def derivative(a, h):\n",
    "    '''\n",
    "    find derivative of a 1D functions given on uniform grid x\n",
    "    a.shape = (N_x)\n",
    "    h = grid spacing\n",
    "    '''\n",
    "    d_a = (jnp.roll(a, -1, axis=0) - jnp.roll(a, 1, axis=0)) / (2*h)\n",
    "    d_a = d_a.at[0].set((-3*a[0]/2 + 2*a[1] - a[2]/2)/h) # 1/2\t−2\t3/2\n",
    "    d_a = d_a.at[-1].set((a[-3]/2 - 2*a[-2] + 3*a[-1]/2)/h) # 1/2\t−2\t3/2\n",
    "    return d_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRuBHc6Phxrp"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optim = optax.adam(learning_rate)\n",
    "\n",
    "def u_update_step(carry, i):\n",
    "    y, u, a, b, f, C_f, opt_state = carry\n",
    "    d_y = grad(upper_bound)(y, u, a, b, f, C_f)\n",
    "    y_update, opt_state = optim.update(d_y, opt_state, y)\n",
    "    y = y + y_update\n",
    "    return [y, u, a, f, C_f, opt_state], upper_bound(y, u, a, b, f, C_f)\n",
    "\n",
    "def u_optimize(y, u, a, b, f, C_f, opt_state, N_sweeps):\n",
    "    carry = [y, u, a, b, f, C_f, opt_state]\n",
    "    i = jnp.arange(N_sweeps)\n",
    "    carry, loss = scan(u_update_step, carry, i)\n",
    "    return carry[0], loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxMCL36bfJfO"
   },
   "outputs": [],
   "source": [
    "def upper_bound(y, u, a, b, f, C_f, N_points=100):\n",
    "    dy = derivative(y, 1 / N_points)\n",
    "    du = derivative(u, 1 / N_points)\n",
    "    return jnp.sqrt(jnp.trapz((y - a * du)**2 / a, dx=1/N_points)) + C_f * jnp.sqrt(jnp.trapz((f + dy - b * u)**2, dx=1/N_points))\n",
    "\n",
    "def estimate_upper_bound(model, input, N_sweeps, lr=1e-4, N_points=100):\n",
    "    output = model(input.reshape(1,3,-1))\n",
    "    u = output[0, :]\n",
    "    a, b, f = input[0, :], input[1, :], input[2, :]\n",
    "    C_f = jnp.max(a, axis=1) / jnp.pi\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "\n",
    "    opt_state = optim.init(a*derivative(u, 1 / N_points))\n",
    "    y, history = u_optimize(a*derivative(u, 1 / N_points), u, a, b, f, C_f, opt_state, N_sweeps)\n",
    "\n",
    "    u_bound = upper_bound(y, u, a, b, f, C_f, opt_state, N_sweeps)\n",
    "    return u_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAgYA_Q6VSJC"
   },
   "outputs": [],
   "source": [
    "def finetuning(iterations, N_new_samples, model_name, params_of_model, params_of_learning, fine_params, N_points=100):\n",
    "    dataset = train_dataset(fine_params['N_samples_train'])\n",
    "    val_dataset = validation_dataset(fine_params['N_samples_val'])\n",
    "\n",
    "    model = Model(model_name, params_of_model, params_of_learning)\n",
    "    val_loss = []\n",
    "    \n",
    "    model.trainer(dataset)\n",
    "    val_loss.append(compute_loss(model.model, val_dataset[0], val_dataset[1]))\n",
    "\n",
    "    count = 0\n",
    "    new_dataset = [jnp.zeros((fine_params['tune_step'], 3, N_points)), jnp.zeros((fine_params['tune_step'], N_points))]\n",
    "\n",
    "    for it in range(iterations):\n",
    "        features = generator(random.PRNGKey(it))\n",
    "        u_bound = estimate_upper_bound(model.model, features, 1000)\n",
    "        if u_bound < 1e-2:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            x = jnp.linspace(0, 1, N_points)\n",
    "            F = [features[:,0], jnp.zeros_like(x), features[:,1], features[:,2]]\n",
    "            BCs = [0, 0]\n",
    "            \n",
    "            solution = solvers.solve_BVP(N_points, F, BCs)\n",
    "            new_dataset[0][count] = features\n",
    "            new_dataset[1][count] = solution\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        if count // fine_params['tune_step'] == 0:\n",
    "            model.fine_trainer(new_dataset, dataset)\n",
    "            dataset[0] = jnp.concatenate([dataset[0],new_dataset[0]],axis=0)\n",
    "            dataset[1] = jnp.concatenate([dataset[1],new_dataset[1]],axis=0)\n",
    "            val_loss.append(compute_loss(model.model, val_dataset[0], val_dataset[1]))\n",
    "\n",
    "        if dataset[0].shape[0] >= (N_new_samples  + fine_params['N_samples_train']):\n",
    "            break\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evjfWG2ZbPYB"
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# FNO\n",
    "##############################################\n",
    "N_layers_FNO = 5\n",
    "n_modes = 12\n",
    "encoder_shapes = [3, 64]\n",
    "decoder_shapes = [64, 128, 1]\n",
    "FNO_shapes = [encoder_shapes[-1], ] * N_layers_FNO\n",
    "spatial_shapes = [n_modes] * N_layers_FNO\n",
    "\n",
    "model_params_FNO = {\n",
    "    'encoder_shapes': encoder_shapes,\n",
    "    'decoder_shapes': decoder_shapes,\n",
    "    'FNO_shapes': FNO_shapes,\n",
    "    'spatial_shapes': spatial_shapes,\n",
    "}\n",
    "\n",
    "##############################################\n",
    "# DilResNet\n",
    "##############################################\n",
    "channels = [3, 32, 1]\n",
    "n_cells = 7\n",
    "\n",
    "model_params_DilResNet = {\n",
    "    'channels': channels,\n",
    "    'n_cells': n_cells\n",
    "}\n",
    "\n",
    "##############################################\n",
    "# Params of learning\n",
    "##############################################\n",
    "train_params = {\n",
    "    'batch_size': 32, \n",
    "    'learning_rate': 1e-3, \n",
    "    'epochs': 50,\n",
    "    'finetuning_epochs': 10,\n",
    "    'finetuning_lr': 1e-3\n",
    "}\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Params of finetuning\n",
    "##############################################\n",
    "fine_params = {\n",
    "    'N_samples_train': 50, \n",
    "    'N_samples_val': 100, \n",
    "    'tune_step': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNCkwDb_pBBo",
    "outputId": "a86e7af6-7140-4aff-8b59-d6162c274a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:49,  1.00it/s]\n",
      " 99%|█████████▉| 99/100 [01:38<00:01,  1.01s/it]"
     ]
    }
   ],
   "source": [
    "finetuning(200, 100, 'DilResNet', model_params_DilResNet, train_params, fine_params)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
