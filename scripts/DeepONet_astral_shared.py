import jax.numpy as jnp
import matplotlib.pyplot as plt
import numpy as np
import sympy as sp
import equinox as eqx
from jax.nn import gelu
from scipy.special import roots_legendre

import itertools
import time
import optax

from jax.lax import scan
from jax import random, jit, vmap, grad
import os

os.environ['CUDA_VISIBLE_DEVICES'] = '4'
## load the elliptic dataset generated by random sin
def np_random_sin(x, amplitudes, k=2):
    N_terms = amplitudes.shape[0]
    frequencies = sorted([*itertools.product(range(1, N_terms + 1), repeat=2)], key=lambda x: sum(x))[:N_terms]
    a = 0
    for n, (i, j) in enumerate(frequencies):
        a += amplitudes[n] * (sp.sin(sp.pi * i * x[0]) + sp.sin(sp.pi * j * x[1]) )/ (
                1 + (sp.pi * i) ** 2 + (sp.pi * j) ** 2) ** k
    return a


def get_coordinates():
    N_mesh = 32
    coords_train = jnp.linspace(0, 1, N_mesh)
    coords_train = jnp.stack(jnp.meshgrid(coords_train, coords_train), 2)
    coords_train = coords_train.reshape(-1, 2)

    N_mesh_eval = 32

    x_, weights_ = roots_legendre(N_mesh_eval)
    x_ = jnp.array((x_ + 1) / 2)
    weights_ = jnp.array(weights_).reshape(1, -1)
    coords_legendre = jnp.stack(jnp.meshgrid(x_, x_), 2)
    coords_legendre = coords_legendre.reshape(-1, 2)

    coords_eval = jnp.linspace(0, 1, N_mesh_eval)
    coords_eval = jnp.stack(jnp.meshgrid(coords_eval, coords_eval), 2)
    coords_eval = coords_eval.reshape(-1, 2)

    return coords_train, weights_, coords_legendre, coords_eval


def get_sample(key, coords_train, weights_, coords_legendre, coords_eval):
    keys = random.split(key)
    N_terms = 10
    amplitudes = random.normal(keys[0], (N_terms,))
    amplitudes_ = random.normal(keys[1], (N_terms,))

    amplitudes = amplitudes/abs(amplitudes).max()
    amplitudes_ = amplitudes_/abs(amplitudes_).max()

    x, y = sp.Symbol("x"), sp.Symbol("y")

    a_ = np_random_sin([x, y], amplitudes, k=1)
    A_ = sp.lambdify([x, y], a_, 'jax')
    A = lambda x: A_(x[0], x[1])

    N_mesh = 32
    x_ = jnp.linspace(0, 1, N_mesh)
    coords = jnp.stack(jnp.meshgrid(x_, x_), 2)
    val_A = A(coords.T)
    min_a, max_a = jnp.min(val_A).item(), jnp.max(val_A).item()

    a = (5 * (a_ - min_a) / (max_a - min_a) + 1)
    dx_a = a.diff("x")
    dy_a = a.diff("y")

    phi = sp.sin(sp.pi * x) * sp.sin(sp.pi * y) * np_random_sin([x, y], amplitudes_, k=1)
    f = - (a * phi.diff("x")).diff("x") - (a * phi.diff("y")).diff("y")

    rhs_ = sp.lambdify([x, y], f, 'jax')
    a_ = sp.lambdify([x, y], a, 'jax')
    dx_a_ = sp.lambdify([x, y], dx_a, 'jax')
    dy_a_ = sp.lambdify([x, y], dy_a, 'jax')

    sol_ = sp.lambdify([x, y], phi, 'jax')
    dx_sol_ = sp.lambdify([x, y], phi.diff('x'), 'jax')
    dy_sol_ = sp.lambdify([x, y], phi.diff('y'), 'jax')

    rhs = lambda x: rhs_(x[0], x[1])
    a = lambda x: a_(x[0], x[1])
    dx_a = lambda x: dx_a_(x[0], x[1])
    dy_a = lambda x: dy_a_(x[0], x[1])
    sol = lambda x: sol_(x[0], x[1])
    dx_sol = lambda x: dx_sol_(x[0], x[1])
    dy_sol = lambda x: dy_sol_(x[0], x[1])

    N_mesh = 32
    x = jnp.linspace(0, 1, N_mesh)
    coords = jnp.stack(jnp.meshgrid(x, x), 2)
    C_F = 1 / (jnp.sqrt(jnp.min(a(coords.T))) * jnp.pi * 2)

    a_train = a(coords_train.T).T
    rhs_train = rhs(coords_train.T).T
    dx_a_train = dx_a(coords_train.T).T
    dy_a_train = dy_a(coords_train.T).T
    sol_train = sol(coords_train.T).T

    a_eval_legendre = a(coords_legendre.T).T
    dx_sol_eval_legendre = dx_sol(coords_legendre.T).T
    dy_sol_eval_legendre = dy_sol(coords_legendre.T).T
    rhs_legendre = rhs(coords_legendre.T).T
    a_legendre = a(coords_legendre.T).T

    sol_eval = sol(coords_eval.T).T

    return a_train, rhs_train, dx_a_train, dy_a_train, a_eval_legendre, dx_sol_eval_legendre, dy_sol_eval_legendre, rhs_legendre, a_legendre, sol_train, sol_eval, C_F


coords_train, weights_, coords_legendre, coords_eval = get_coordinates()
a_train, rhs_train, dx_a_train, dy_a_train, a_eval_legendre, dx_sol_eval_legendre, dy_sol_eval_legendre, rhs_legendre, a_legendre, sol_train, sol_eval, C_F = [], [], [], [], [], [], [], [], [], [], [], []
key = random.PRNGKey(23)
keys = random.split(key, 1000)
for key in keys:
    a_train_, rhs_train_, dx_a_train_, dy_a_train_, a_eval_legendre_, dx_sol_eval_legendre_, dy_sol_eval_legendre_, rhs_legendre_, a_legendre_, sol_train_, sol_eval_, C_F_ = get_sample(
        key, coords_train, weights_, coords_legendre, coords_eval)
    a_train.append(a_train_)
    rhs_train.append(rhs_train_)
    dx_a_train.append(dx_a_train_)
    dy_a_train.append(dy_a_train_)
    a_eval_legendre.append(a_eval_legendre_)
    dx_sol_eval_legendre.append(dx_sol_eval_legendre_)
    dy_sol_eval_legendre.append(dy_sol_eval_legendre_)
    rhs_legendre.append(rhs_legendre_)
    a_legendre.append(a_legendre_)
    sol_train.append(sol_train_)
    sol_eval.append(sol_eval_)
    C_F.append(C_F_)

a_train = jnp.stack(a_train)
rhs_train = jnp.stack(rhs_train)
dx_a_train = jnp.stack(dx_a_train)
dy_a_train = jnp.stack(dy_a_train)
sol_train = jnp.stack(sol_train)
a_eval_legendre = jnp.stack(a_eval_legendre)
dx_sol_eval_legendre = jnp.stack(dx_sol_eval_legendre)
dy_sol_eval_legendre = jnp.stack(dy_sol_eval_legendre)
rhs_legendre = jnp.stack(rhs_legendre)
a_legendre = jnp.stack(a_legendre)
sol_eval = jnp.stack(sol_eval)
C_F = jnp.array(C_F)


class Trunk(eqx.Module):
    matrices: list
    biases: list

    def __init__(self, N_features, N_layers, key):
        keys = random.split(key, N_layers + 1)
        features = [N_features[0], ] + [N_features[1], ] * (N_layers - 1) + [N_features[-1], ]
        self.matrices = [random.normal(key, (f_in, f_out)) / jnp.sqrt((f_in + f_out) / 2) for f_in, f_out, key in
                         zip(features[:-1], features[1:], keys)]
        keys = random.split(keys[-1], N_layers + 1)
        self.biases = [random.normal(key, (f_out,)) for f_in, f_out, key in zip(features[:-1], features[1:], keys)]

    def __call__(self, x, B):
        f = jnp.concatenate([jnp.cos(B @ x), jnp.sin(B @ x)], 0)
        f = f @ self.matrices[0] + self.biases[0]
        for i in range(1, len(self.matrices)):
            f = gelu(f)
            f = f @ self.matrices[i] + self.biases[i]
        return f
        # return jnp.sum(b * f)


class DeepONet(eqx.Module):
    matrices_b: list
    biases_b: list

    def __init__(self, N_features, N_layers, N_branch, key):
        keys = random.split(key, N_layers + 1)
        features_ = [N_branch, ] + [N_features[-1], ] * N_layers
        self.matrices_b = [random.normal(key, (f_in, f_out)) / jnp.sqrt(f_in / 2) for f_in, f_out, key in
                           zip(features_[:-1], features_[1:], keys)]
        keys = random.split(keys[-1], N_layers + 1)
        self.biases_b = [jnp.zeros((f_out,)) for f_in, f_out, key in zip(features_[:-1], features_[1:], keys)]

    def __call__(self, x, B, features, trunk):
        f = trunk(x, B)

        b = features @ self.matrices_b[0] + self.biases_b[0]
        for i in range(1, len(self.matrices_b)):
            b = gelu(b)
            b = b @ self.matrices_b[i] + self.biases_b[i]
        return jnp.sin(jnp.pi * x[0]) * jnp.sin(jnp.pi * x[1]) * jnp.sum(b * f)
        # return jnp.sum(b * f)


class DeepONetu(eqx.Module):
    matrices_b: list
    biases_b: list

    def __init__(self, N_features, N_layers, N_branch, key):
        keys = random.split(key, N_layers + 1)
        features_ = [N_branch, ] + [N_features[-1], ] * N_layers
        self.matrices_b = [random.normal(key, (f_in, f_out)) / jnp.sqrt(f_in / 2) for f_in, f_out, key in
                           zip(features_[:-1], features_[1:], keys)]
        keys = random.split(keys[-1], N_layers + 1)
        self.biases_b = [jnp.zeros((f_out,)) for f_in, f_out, key in zip(features_[:-1], features_[1:], keys)]

    def __call__(self, x, B, features, trunk):
        f = trunk(x, B)

        b = features @ self.matrices_b[0] + self.biases_b[0]
        for i in range(1, len(self.matrices_b)):
            b = gelu(b)
            b = b @ self.matrices_b[i] + self.biases_b[i]
        return jnp.sum(b * f)


class DeepONet3(eqx.Module):
    models: list
    beta: jnp.array

    def __init__(self, N_features, N_layers, N_branch, key):
        keys = random.split(key, 4)
        self.models = [DeepONetu(N_features, N_layers, N_branch, key) for key in keys[:2]]
        self.models.append(DeepONet(N_features, N_layers, N_branch, keys[2]))
        self.beta = jnp.array([1.0, ])
        self.models.append(Trunk(N_features, N_layers, keys[-1]))

    def __call__(self, x, B, features, i):
        return self.models[i](x, B, features, self.models[-1])


def compute_loss_astral(model, coordinates, rhs, a, B, C_F, features):
    flux = vmap(grad(model, argnums=0), in_axes=(0, None, None, None), out_axes=1)(coordinates, B, features, 2)
    dx_y = [vmap(grad(model, argnums=0), in_axes=(0, None, None, None), out_axes=1)(coordinates, B, features, i)[i] for
            i in [0, 1]]
    y1 = vmap(model, in_axes=(0, None, None, None))(coordinates, B, features, 0)
    y2 = vmap(model, in_axes=(0, None, None, None))(coordinates, B, features, 1)
    loss = (1 + model.beta[0] ** 2) * (
            C_F ** 2 * (rhs + dx_y[0] + dx_y[1]) ** 2 + ((a * flux[0] - y1) ** 2 + (a * flux[1] - y2) ** 2) / (
            a * model.beta[0] ** 2))
    return jnp.linalg.norm(loss)


def compute_loss_data(model, coordinates, rhs, a, B, C_F, features, targets):
    sol = vmap(model, in_axes=(0, None, None, None))(coordinates, B, features, 2)
    loss = (sol - targets)
    return jnp.linalg.norm(loss)


def compute_loss(model, coordinates, rhs, a, B, C_F, features, targets):
    return jnp.mean(
        vmap(compute_loss_astral, in_axes=(None, None, 0, 0, None, 0, 0))(model, coordinates, rhs, a, B, C_F,
                                                                          features))
    # +jnp.mean(vmap(compute_loss_data, in_axes=(None, None, 0, 0, None, 0, 0, 0))(model, coordinates, rhs, a, B, C_F, features,
    #                                                                 targets))


def compute_upper_bound(model, coordinates, weights, rhs, a, B, C_F, features):
    flux = vmap(grad(model, argnums=0), in_axes=(0, None, None, None), out_axes=1)(coordinates, B, features, 2)
    dx_y = [vmap(grad(model, argnums=0), in_axes=(0, None, None, None), out_axes=1)(coordinates, B, features, i)[i] for
            i in [0, 1]]
    y1 = vmap(model, in_axes=(0, None, None, None))(coordinates, B, features, 0)
    y2 = vmap(model, in_axes=(0, None, None, None))(coordinates, B, features, 1)
    integrand = (1 + model.beta[0] ** 2) * (
            C_F ** 2 * (rhs + dx_y[0] + dx_y[1]) ** 2 + ((a * flux[0] - y1) ** 2 + (a * flux[1] - y2) ** 2) / (
            a * model.beta[0] ** 2))
    l = jnp.sum(jnp.sum(integrand.reshape(weights.size, weights.size) * weights, axis=1) * weights[0]) / 4
    return l


def compute_error_energy_norm(model, coordinates, a, dx_sol, dy_sol, weights, B, features):
    flux = vmap(grad(model, argnums=0), in_axes=(0, None, None, None), out_axes=1)(coordinates, B, features, 2)
    integrand = a * ((flux[0] - dx_sol) ** 2 + (flux[1] - dy_sol) ** 2)
    l = jnp.sum(jnp.sum(integrand.reshape(weights.size, weights.size) * weights, axis=1) * weights[0]) / 4
    return l


compute_loss_and_grads = eqx.filter_value_and_grad(compute_loss)


def make_step_scan(carry, ind, optim, M):
    model, coordinates, rhs, a, B, C_F, features, targets, opt_state = carry
    loss, grads = compute_loss_and_grads(model, coordinates[ind[:M]], rhs[ind[M:], :][:, ind[:M]],
                                         a[ind[M:], :][:, ind[:M]], B, C_F[ind[M:]], features[ind[M:]],
                                         targets[ind[M:]][:, ind[:M]])
    updates, opt_state = optim.update(grads, opt_state, eqx.filter(model, eqx.is_array))
    model = eqx.apply_updates(model, updates)
    return [model, coordinates, rhs, a, B, C_F, features, targets, opt_state], loss


N_mesh = 16
N_epochs = 500
M = 12 * 12
N_train = 800
N_batch = 100

N_modes = 400

_, _, vt_a = jnp.linalg.svd(a_legendre[:N_train])
_, _, vt_rhs = jnp.linalg.svd(rhs_legendre[:N_train])

features = [X @ (y.T)[:, :N_modes] for X, y in zip([a_train, rhs_train], [vt_a, vt_rhs])]
features = jnp.concatenate(features, 1)

# features = [X.reshape(X.shape[0], 64, 64)[:, ::16, ::16].reshape(X.shape[0], -1) for X in [a_train, rhs_train]]
# features = jnp.concatenate(features, 1)

N_fourier_features = 25
key = random.PRNGKey(23)
B = random.normal(key, (N_fourier_features, 2)) * 10
N_features = [2 * N_fourier_features, 100, 50]
N_layers = 10
N_branch = features.shape[1]
model = DeepONet3(N_features, N_layers, N_branch, key)

learning_rate = 1e-4
N_drop = 10000
gamma = 0.95
sc = optax.exponential_decay(learning_rate, N_drop, gamma)
optim = optax.lion(learning_rate=sc)
opt_state = optim.init(eqx.filter(model, eqx.is_array))

carry = [model, coords_train, rhs_train, a_train, B, C_F, features, sol_train, opt_state]

make_step_scan_ = lambda a, b: make_step_scan(a, b, optim, M)
keys = random.split(key, 2)
for i in range(30):
    keys = random.split(keys[-1], 3)
    inds = random.choice(keys[0], N_mesh * N_mesh, (N_epochs, M))
    inds_ = random.choice(keys[1], N_train, (N_epochs, N_batch))
    inds = jnp.concatenate([inds, inds_], 1)
    carry, loss = scan(make_step_scan_, carry, inds)
    print(loss[-1])
    model = carry[0]
    predicted = vmap(lambda x: vmap(model, in_axes=(0, None, None, None))(coords_train, B, x, 2))(features)
    relative_error = jnp.linalg.norm(predicted - sol_legendre, axis=1) / jnp.linalg.norm(sol_legendre, axis=1)
    energy_norm = jnp.sqrt(
        vmap(compute_error_energy_norm, in_axes=(None, None, 0, 0, 0, None, None, 0))(model, coords_legendre,
                                                                                      a_eval_legendre,
                                                                                      dx_sol_eval_legendre,
                                                                                      dy_sol_eval_legendre, weights_, B,
                                                                                      features))
    upper_bound = jnp.sqrt(
        vmap(compute_upper_bound, in_axes=(None, None, None, 0, 0, None, 0, 0))(model, coords_legendre, weights_,
                                                                                rhs_legendre, a_legendre, B, C_F,
                                                                                features))

    print(
        f"Epochs {(i + 1) * N_epochs}, Train errors {jnp.mean(relative_error[:N_train])} +/- {jnp.sqrt(jnp.var(relative_error[:N_train]))}, {jnp.mean(energy_norm[:N_train])} +/- {jnp.sqrt(jnp.var(energy_norm[:N_train]))}, {jnp.mean(upper_bound[:N_train])} +/- {jnp.sqrt(jnp.var(upper_bound[:N_train]))}")
    print(
        f"Epochs {(i + 1) * N_epochs}, Test errors {jnp.mean(relative_error[N_train:])} +/- {jnp.sqrt(jnp.var(relative_error[N_train:]))}, {jnp.mean(energy_norm[N_train:])} +/- {jnp.sqrt(jnp.var(energy_norm[N_train:]))}, {jnp.mean(upper_bound[N_train:])} +/- {jnp.sqrt(jnp.var(upper_bound[N_train:]))}")

    # print(
    #     f"Epochs {(i + 1) * N_epochs}, Train errors {jnp.mean(relative_error[:N_train])} +/- {jnp.sqrt(jnp.var(relative_error[:N_train]))}")
    # print(
    #     f"Epochs {(i + 1) * N_epochs}, Test errors {jnp.mean(relative_error[N_train:])} +/- {jnp.sqrt(jnp.var(relative_error[N_train:]))}")

fig, ax = plt.subplots(1, 3, figsize=(9, 3))
ax[0].contourf(predicted[1, :].reshape(16, 16))
ax[1].contourf(sol_eval[1, :].reshape(16, 16))
ax[2].contourf(abs(sol_eval[1, :].reshape(16, 16) - predicted[1, :].reshape(16, 16)))
fig.savefig('astral.jpg')

fig, ax = plt.subplots(1, 3, figsize=(9, 3))
ax[0].contourf(predicted[-1, :].reshape(16, 16))
ax[1].contourf(sol_eval[-1, :].reshape(16, 16))
ax[2].contourf(abs(sol_eval[-1, :].reshape(16, 16) - predicted[-1, :].reshape(16, 16)))
fig.savefig('astral2.jpg')
